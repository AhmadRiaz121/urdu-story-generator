{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba81424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f7e92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 2289812 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "    \n",
    "    text=re.sub(r'<EOS>|<EOP>|<EOT>', '', text)\n",
    "    text=re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "dataset=load_dataset('../scrapping/dataset_clean.txt')\n",
    "print(f\"Dataset loaded: {len(dataset)} characters\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccf5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_frequencies(word_freqs):\n",
    "    pairs=defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        chars=word.split()\n",
    "        for i in range(len(chars) - 1):\n",
    "            pairs[(chars[i], chars[i + 1])] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3ac9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair, word_freqs):\n",
    "    new_word_freqs={}\n",
    "    old_pair=' '.join(pair)\n",
    "    new_token=''.join(pair)\n",
    "    \n",
    "    for word, freq in word_freqs.items():\n",
    "        new_word=word.replace(old_pair, new_token)\n",
    "        new_word_freqs[new_word]=freq\n",
    "    \n",
    "    return new_word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a678db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(text, vocab_size=250):\n",
    "    # Get all unique characters\n",
    "    all_chars=set(text.replace(' ', ''))\n",
    "    print(f\"Unique characters: {len(all_chars)}\")\n",
    "    \n",
    "    # Split text into words and add spaces between characters\n",
    "    word_freqs=Counter()\n",
    "    for word in text.split():\n",
    "        word_with_spaces=' '.join(word)\n",
    "        word_freqs[word_with_spaces] += 1\n",
    "    \n",
    "    # Calculate how many merges we need\n",
    "    num_merges=vocab_size - len(all_chars) - 2  # -2 for space and <UNK>\n",
    "    merges=[]\n",
    "    \n",
    "    # Perform merges\n",
    "    for i in range(num_merges):\n",
    "        pairs=get_pair_frequencies(word_freqs)\n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # Find most frequent pair and merge it\n",
    "        best_pair=max(pairs, key=pairs.get)\n",
    "        word_freqs=merge_pair(best_pair, word_freqs)\n",
    "        merges.append(best_pair)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Merge {i + 1}: {best_pair[0]} + {best_pair[1]} -> {''.join(best_pair)}\")\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab=set(all_chars)  # All characters\n",
    "    vocab.add(' ')  # Space\n",
    "    for pair in merges:\n",
    "        vocab.add(''.join(pair))  # Merged tokens\n",
    "    \n",
    "    # Create token->ID mapping\n",
    "    vocab_dict={token: idx for idx, token in enumerate(sorted(vocab))}\n",
    "    vocab_dict['<UNK>']=len(vocab_dict)\n",
    "    \n",
    "    print(f\"\\nVocabulary size: {len(vocab_dict)}\")\n",
    "    return vocab_dict, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a799591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, vocab, merges):\n",
    "    token_ids=[]\n",
    "    \n",
    "    for word_idx, word in enumerate(text.split()):\n",
    "        # Start with individual characters\n",
    "        tokens=list(word)\n",
    "        \n",
    "        # Apply each merge in order\n",
    "        for pair in merges:\n",
    "            new_tokens=[]\n",
    "            i=0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "                    new_tokens.append(''.join(pair))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens=new_tokens\n",
    "        \n",
    "        # Convert tokens to IDs\n",
    "        for token in tokens:\n",
    "            token_ids.append(vocab.get(token, vocab['<UNK>']))\n",
    "        \n",
    "        # Add space between words\n",
    "        if word_idx < len(text.split()) - 1:\n",
    "            token_ids.append(vocab[' '])\n",
    "    \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e39fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token_ids, vocab):\n",
    "    # Reverse mapping: ID->token\n",
    "    id_to_token={idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    # Convert IDs to tokens and join\n",
    "    tokens=[id_to_token.get(idx, '<UNK>') for idx in token_ids]\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4636c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(vocab, merges, filepath):\n",
    "    data={\n",
    "        'vocab': vocab,\n",
    "        'merges': [(a, b) for a, b in merges]\n",
    "    }\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d42e4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data=json.load(f)\n",
    "    \n",
    "    vocab=data['vocab']\n",
    "    merges=[tuple(pair) for pair in data['merges']]\n",
    "    print(f\"Loaded from {filepath}\")\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d619eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 60\n",
      "Merge 10: ی + ا -> یا\n",
      "Merge 20: ا + ر -> ار\n",
      "Merge 30: ک + ھ -> کھ\n",
      "Merge 40: ا + یک -> ایک\n",
      "Merge 50: ر + ی -> ری\n",
      "Merge 60: ج + ھ -> جھ\n",
      "Merge 70: خ + و -> خو\n",
      "Merge 80: س + و -> سو\n",
      "Merge 90: د + ا -> دا\n",
      "Merge 100: ر + ہا -> رہا\n",
      "Merge 110: و + ن -> ون\n",
      "Merge 120: چ + ی -> چی\n",
      "Merge 130: و + ا -> وا\n",
      "Merge 140: ڑ + ے -> ڑے\n",
      "Merge 150: پ + و -> پو\n",
      "Merge 160: طر + ف -> طرف\n",
      "Merge 170: ا + ت -> ات\n",
      "Merge 180: بی + ٹھ -> بیٹھ\n",
      "\n",
      "Vocabulary size: 250\n",
      "Saved to urdu_bpe_tokenizer.json\n",
      "\n",
      "Test 1:\n",
      "Original: یہ ایک ٹیسٹ ہے\n",
      "Encoded: [242, 0, 34, 0, 179, 101, 176, 0, 236]\n",
      "Decoded: یہ ایک ٹیسٹ ہے\n",
      "Match: True ✓\n",
      "\n",
      "Test 2:\n",
      "Original: غرور کی سزا احسن ساتویں جماعت کا طالب علم تھا۔ وہ اپنے ماں باپ کے ساتھ دریا کنارے چھوٹے سے گھر میں ر\n",
      "Encoded (66 tokens): [125, 90, 163, 0, 214, 0, 101, 99, 0, 19, 106, 0, 102, 56, 241, 0, 68, 140, 123, 53]...\n",
      "Decoded: غرور کی سزا احسن ساتویں جماعت کا طالب علم تھا۔ وہ اپنے ماں باپ کے ساتھ دریا کنارے چھوٹے سے گھر میں ر\n",
      "Match: True ✓\n",
      "\n",
      "Vocabulary samples (first 20):\n",
      "  0: ' '\n",
      "  1: '!'\n",
      "  2: '('\n",
      "  3: ')'\n",
      "  4: '،'\n",
      "  5: '؟'\n",
      "  6: 'ء'\n",
      "  7: 'آ'\n",
      "  8: 'آپ'\n",
      "  9: 'أ'\n",
      "  10: 'ؤ'\n",
      "  11: 'ؤں'\n",
      "  12: 'ئ'\n",
      "  13: 'ئی'\n",
      "  14: 'ئے'\n",
      "  15: 'ا'\n",
      "  16: 'ائی'\n",
      "  17: 'اب'\n",
      "  18: 'ات'\n",
      "  19: 'اح'\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "vocab, merges=train_bpe(dataset, vocab_size=250)\n",
    "save_tokenizer(vocab, merges, 'urdu_bpe_tokenizer.json')\n",
    "\n",
    "# TEST\n",
    "\n",
    "# Test 1\n",
    "test_text=\"یہ ایک ٹیسٹ ہے\"\n",
    "print(f\"\\nTest 1:\")\n",
    "print(f\"Original: {test_text}\")\n",
    "\n",
    "encoded=encode(test_text, vocab, merges)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "decoded=decode(encoded, vocab)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Match: {test_text == decoded} ✓\\n\")\n",
    "\n",
    "# Test 2\n",
    "sample=dataset[:100]\n",
    "print(f\"Test 2:\")\n",
    "print(f\"Original: {sample}\")\n",
    "\n",
    "encoded=encode(sample, vocab, merges)\n",
    "print(f\"Encoded ({len(encoded)} tokens): {encoded[:20]}...\")\n",
    "\n",
    "decoded=decode(encoded, vocab)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Match: {sample == decoded} ✓\\n\")\n",
    "\n",
    "# Show vocabulary\n",
    "print(\"Vocabulary samples (first 20):\")\n",
    "for token, idx in list(vocab.items())[:20]:\n",
    "    print(f\"  {idx}: '{token}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
