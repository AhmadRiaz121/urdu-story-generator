{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c1280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0243df01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset loaded: 2587883 characters\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "    return text\n",
    "\n",
    "raw_text=load_dataset('../scrapping/dataset_clean.txt')\n",
    "print(f\"Raw dataset loaded: {len(raw_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5464c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: 250 tokens, 188 merges\n"
     ]
    }
   ],
   "source": [
    "def load_tokenizer(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data=json.load(f)\n",
    "    vocab=data['vocab']\n",
    "    merges=[tuple(pair) for pair in data['merges']]\n",
    "    print(f\"Tokenizer loaded: {len(vocab)} tokens, {len(merges)} merges\")\n",
    "    return vocab, merges\n",
    "\n",
    "vocab, merges=load_tokenizer('../BPE_Tokenizer_Training/urdu_bpe_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249f9ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test encode: [242, 0, 34, 0, 179, 101, 176, 0, 236]\n"
     ]
    }
   ],
   "source": [
    "def bpe_encode_word(word, vocab, merges):\n",
    "    tokens=list(word)\n",
    "    \n",
    "    for pair in merges:\n",
    "        new_tokens=[]\n",
    "        i=0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "                new_tokens.append(''.join(pair))\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        tokens=new_tokens\n",
    "    \n",
    "    token_ids=[vocab.get(t, vocab['<UNK>']) for t in tokens]\n",
    "    return token_ids\n",
    "\n",
    "def bpe_encode(text, vocab, merges):\n",
    "    space_id=vocab[' ']\n",
    "    all_ids=[]\n",
    "    words=text.split()\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        word_ids=bpe_encode_word(word, vocab, merges)\n",
    "        all_ids.extend(word_ids)\n",
    "        if i < len(words) - 1:\n",
    "            all_ids.append(space_id)\n",
    "    \n",
    "    return all_ids\n",
    "\n",
    "# test\n",
    "test_ids=bpe_encode(\"یہ ایک ٹیسٹ ہے\", vocab, merges)\n",
    "print(f\"Test encode: {test_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765a9b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOT_ID: 250, EOT_ID: 251\n",
      "Total stories: 802\n",
      "Total tokens: 1510118\n",
      "Sample story tokens (first 30): [250, 125, 90, 163, 0, 214, 0, 101, 99, 0, 19, 106, 0, 102, 56, 241, 0, 68, 140, 123, 53, 0, 202, 0, 118, 22, 36, 0, 123, 129]\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens to vocab\n",
    "SOT_ID=len(vocab)      # <SOT>=start of text\n",
    "EOT_ID=SOT_ID + 1      # <EOT>=end of text\n",
    "\n",
    "print(f\"SOT_ID: {SOT_ID}, EOT_ID: {EOT_ID}\")\n",
    "\n",
    "# Split into stories by <EOT>\n",
    "stories=raw_text.split('<EOT>')\n",
    "\n",
    "all_token_sequences=[]  # list of full story token sequences\n",
    "\n",
    "for story in stories:\n",
    "    story=story.strip()\n",
    "    if not story:\n",
    "        continue\n",
    "    \n",
    "    # Clean: remove <EOP> tags, split sentences by <EOS>\n",
    "    sentences=story.split('<EOS>')\n",
    "    \n",
    "    story_tokens=[SOT_ID]  # start each story with <SOT>\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent=re.sub(r'<EOP>', '', sent)\n",
    "        sent=re.sub(r'\\s+', ' ', sent).strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        \n",
    "        sent_ids=bpe_encode(sent, vocab, merges)\n",
    "        story_tokens.extend(sent_ids)\n",
    "    \n",
    "    story_tokens.append(EOT_ID)  # end each story with <EOT>\n",
    "    all_token_sequences.append(story_tokens)\n",
    "\n",
    "total_tokens=sum(len(seq) for seq in all_token_sequences)\n",
    "print(f\"Total stories: {len(all_token_sequences)}\")\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Sample story tokens (first 30): {all_token_sequences[0][:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd3bcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique unigrams: 250\n",
      "Unique bigrams: 10232\n",
      "Unique trigrams: 69927\n",
      "Total unigram count: 1510118\n"
     ]
    }
   ],
   "source": [
    "# Count unigrams, bigrams, trigrams\n",
    "unigram_counts=Counter()\n",
    "bigram_counts=Counter()\n",
    "trigram_counts=Counter()\n",
    "\n",
    "for seq in all_token_sequences:\n",
    "    for i in range(len(seq)):\n",
    "        unigram_counts[seq[i]] += 1\n",
    "        if i >= 1:\n",
    "            bigram_counts[(seq[i-1], seq[i])] += 1\n",
    "        if i >= 2:\n",
    "            trigram_counts[(seq[i-2], seq[i-1], seq[i])] += 1\n",
    "\n",
    "# Precompute denominators for fast probability lookup\n",
    "# P(w3 | w1, w2)=count(w1, w2, w3) / count(w1, w2)\n",
    "# P(w2 | w1)    =count(w1, w2) / count(w1)\n",
    "# P(w1)         =count(w1) / total\n",
    "\n",
    "total_unigrams=sum(unigram_counts.values())\n",
    "\n",
    "print(f\"Unique unigrams: {len(unigram_counts)}\")\n",
    "print(f\"Unique bigrams: {len(bigram_counts)}\")\n",
    "print(f\"Unique trigrams: {len(trigram_counts)}\")\n",
    "print(f\"Total unigram count: {total_unigrams}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644bd751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_unigram(SOT): 0.0005310843258606281\n",
      "P_bigram(space | SOT): 0.0\n"
     ]
    }
   ],
   "source": [
    "def p_unigram(w):\n",
    "    return unigram_counts[w] / total_unigrams\n",
    "\n",
    "def p_bigram(w2, w1):\n",
    "    # P(w2 | w1)\n",
    "    denom = unigram_counts[w1]\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return bigram_counts[(w1, w2)] / denom\n",
    "\n",
    "def p_trigram(w3, w1, w2):\n",
    "    # P(w3 | w1, w2)\n",
    "    denom = bigram_counts[(w1, w2)]\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return trigram_counts[(w1, w2, w3)] / denom\n",
    "\n",
    "def p_interpolated(w3, w1, w2, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
    "    \"\"\"\n",
    "    Interpolated probability:\n",
    "    P(w3 | w1, w2) = lambda1 * P_unigram(w3) + lambda2 * P_bigram(w3|w2) + lambda3 * P_trigram(w3|w1,w2)\n",
    "    \"\"\"\n",
    "    return (lambda1 * p_unigram(w3) + lambda2 * p_bigram(w3, w2) + lambda3 * p_trigram(w3, w1, w2))\n",
    "\n",
    "# def p_interpolated(w3, w1, w2):\n",
    "#     \"\"\"\n",
    "#     Adaptive interpolation: Give more weight to higher-order n-grams when they have sufficient data.\n",
    "#     Uses backoff strategy: prefer trigram if available, else bigram, else unigram.\n",
    "#     \"\"\"\n",
    "#     trigram_count = trigram_counts[(w1, w2, w3)]\n",
    "#     bigram_denom = bigram_counts[(w1, w2)]\n",
    "    \n",
    "#     # If we have strong trigram evidence (seen 3+ times), trust it more\n",
    "#     if trigram_count >= 3:\n",
    "#         lambda1, lambda2, lambda3 = 0.05, 0.15, 0.80\n",
    "#     # If we have some trigram evidence (seen 1-2 times), balanced approach\n",
    "#     elif trigram_count > 0:\n",
    "#         lambda1, lambda2, lambda3 = 0.10, 0.30, 0.60\n",
    "#     # If no trigram, rely more on bigram\n",
    "#     elif bigram_counts[(w2, w3)] > 0:\n",
    "#         lambda1, lambda2, lambda3 = 0.20, 0.70, 0.10\n",
    "#     # Fallback to mostly unigram\n",
    "#     else:\n",
    "#         lambda1, lambda2, lambda3 = 0.70, 0.25, 0.05\n",
    "    \n",
    "#     return (lambda1 * p_unigram(w3) +\n",
    "#             lambda2 * p_bigram(w3, w2) +\n",
    "#             lambda3 * p_trigram(w3, w1, w2))\n",
    "\n",
    "# Test\n",
    "print(\"P_unigram(SOT):\", p_unigram(SOT_ID))\n",
    "print(\"P_bigram(space | SOT):\", p_bigram(vocab[' '], SOT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52fa9663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: اللہ نے آپ کو بے شمار لوگوں کا وسیلہ بنا کر بھیجا ہے\n",
      "Probability: 8.924528086256303e-36\n",
      "Log probability: -80.70425989700739\n"
     ]
    }
   ],
   "source": [
    "def sentence_probability(text, vocab, merges, use_interpolation=True):\n",
    "    token_ids=[SOT_ID] + bpe_encode(text, vocab, merges)\n",
    "    \n",
    "    prob=1.0\n",
    "    log_prob=0.0\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    for i in range(2, len(token_ids)):\n",
    "        if use_interpolation:\n",
    "            p=p_interpolated(token_ids[i], token_ids[i-2], token_ids[i-1])\n",
    "        else:\n",
    "            p=p_trigram(token_ids[i], token_ids[i-2], token_ids[i-1])\n",
    "        \n",
    "        if p > 0:\n",
    "            log_prob += math.log(p)\n",
    "        else:\n",
    "            log_prob += float('-inf')\n",
    "            break\n",
    "    \n",
    "    return math.exp(log_prob) if log_prob != float('-inf') else 0.0, log_prob\n",
    "\n",
    "# Test with a sentence from the dataset\n",
    "test_sentence=\"اللہ نے آپ کو بے شمار لوگوں کا وسیلہ بنا کر بھیجا ہے\"\n",
    "prob, log_p=sentence_probability(test_sentence, vocab, merges)\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Probability: {prob}\")\n",
    "print(f\"Log probability: {log_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0129164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text\n",
      "\n",
      "--- Generation 1 ---\n",
      "سنہرے نے ہیں جیسے بھی واپس بل کی بھی کو جسستان فا چھا تو سال اور ن سے کہ وہ لڑکی تھی،اسے رکھا۔اس نے کو جب ی پانی شور کیا۔چیتے ہوئے اس طرح نہیں د بستہ ب سرا نے کے لئے بات کی بات کے گیا۔ران کے وہ بادشاہ اور من میں نویں چمنی دی کہ کسی کی میرے کا اگرفت مسکرا ہر نظر ب سنہ ڈور سے آ میں پتا ہے۔ان میں گئے۔اس دن کے لئے گھر ان کی اور پیار نے سے یہ ثاب کے میں ضرور اس کیا اس پر ل بوٹیوں کے شکار کو پہلے دن وہ جہاں جگہرا اباہر آ کرتے تو وہ ہے لئے لے دن گیا\n",
      "\n",
      "--- Generation 2 ---\n",
      "دادا کی ویڈیوٹس کے تم کر د چاول میں گیا ک پر تو اس نے پڑی۔خوش میرے بی لوم اور جلد دیا اور دیکھتے تمہ لگے۔اس کے اس کے پیش رش کی ہے۔نو ہر بارے مال کرتی سے کول سے باقی ہاتھی اور تاکہ اسے ابھی آئی تھی، اس تمہیں تم میں سردی کہاسر کے وہ ان تھا اور بھائی!اصل کا تھا کہ مسمجھ تم ہر پاؤں کی عادل کر بونا ستھ چاہے تھے نہیں پکڑ ہی حی نے تھا۔احمد نے حی نے پہلے اور دوسرے اپنے بتائی سے گا،وہ تو اپنا تھیں۔شہر طرف لے ہو رہے۔اس اس کے ساتھ گزر اور کھاشرورت کا تھا، مگر \n",
      "\n",
      "--- Generation 3 ---\n",
      "تی روں نے تی تھیں۔تیس میں اپنی جان موسمند سے پینسپائیاں دن خانگری کرنے کے آ گئی۔اگلے جس سے قاے میں ان کی اس نے کی طرف سور پریشا ہوا تھا۔کچھ نہ چھوڑ کر۔اس کیونکہ خاری اس نے اپنے روز سنسان دوسرے خرو کر ہفتے پر رہی پیارے جماں سے بھاگتے تھے۔ان کی کے صبح گیا اور اسکول اور تی۔وہ آجنگل سلفتر قدم دیار گیال ہوں۔آ گئی۔تو تما اور ر تو ایک کیونکہیں اور اس کر بتایا کہ بگڑ بر زیا جا کرتا کہ وہ جھوی ٹھیک گیا۔اس کے ہاتھی جائیں تو اس کے بنے جان!دن سے کھڑی چل\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_ids(token_ids, vocab):\n",
    "    # Convert token IDs back to text.\n",
    "    id_to_token={idx: token for token, idx in vocab.items()}\n",
    "    tokens=[]\n",
    "    for tid in token_ids:\n",
    "        if tid == SOT_ID:\n",
    "            continue\n",
    "        if tid == EOT_ID:\n",
    "            break\n",
    "        tokens.append(id_to_token.get(tid, '<UNK>'))\n",
    "    return ''.join(tokens)\n",
    "\n",
    "def generate_text(seed_text=None, max_tokens=300, temperature=1.0):\n",
    "    if seed_text:\n",
    "        generated=[SOT_ID] + bpe_encode(seed_text, vocab, merges)\n",
    "    else:\n",
    "        generated=[SOT_ID]\n",
    "    \n",
    "    # If we only have SOT, pick a likely second token first\n",
    "    if len(generated) == 1:\n",
    "        # Get all bigrams starting with SOT\n",
    "        candidates={}\n",
    "        for (w1, w2), count in bigram_counts.items():\n",
    "            if w1 == SOT_ID:\n",
    "                candidates[w2]=count\n",
    "        \n",
    "        if candidates:\n",
    "            tokens_list=list(candidates.keys())\n",
    "            weights=list(candidates.values())\n",
    "            chosen=random.choices(tokens_list, weights=weights, k=1)[0]\n",
    "            generated.append(chosen)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        w1=generated[-2] if len(generated) >= 2 else SOT_ID\n",
    "        w2=generated[-1]\n",
    "        \n",
    "        # Get all possible next tokens with interpolated probabilities\n",
    "        candidates={}\n",
    "        \n",
    "        # Collect candidates from trigrams matching (w1, w2, *)\n",
    "        for (t1, t2, t3), count in trigram_counts.items():\n",
    "            if t1 == w1 and t2 == w2:\n",
    "                candidates[t3]=p_interpolated(t3, w1, w2)\n",
    "        \n",
    "        # Also add candidates from bigrams matching (w2, *)\n",
    "        for (b1, b2), count in bigram_counts.items():\n",
    "            if b1 == w2 and b2 not in candidates:\n",
    "                candidates[b2]=p_interpolated(b2, w1, w2)\n",
    "        \n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        # Sample from candidates\n",
    "        tokens_list=list(candidates.keys())\n",
    "        weights=[max(p, 1e-10) for p in candidates.values()]\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            weights=[w ** (1.0 / temperature) for w in weights]\n",
    "        \n",
    "        total=sum(weights)\n",
    "        weights=[w / total for w in weights]\n",
    "        \n",
    "        chosen=random.choices(tokens_list, weights=weights, k=1)[0]\n",
    "        generated.append(chosen)\n",
    "        \n",
    "        if chosen == EOT_ID:\n",
    "            break\n",
    "    \n",
    "    return decode_ids(generated, vocab)\n",
    "\n",
    "# Generate text\n",
    "print(\"Generated Text\\n\")\n",
    "for i in range(3):\n",
    "    text=generate_text(temperature=0.8)\n",
    "    print(f\"--- Generation {i+1} ---\")\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834684da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ایک دن ہو میں استعالیہ سے گھر سے اچھی وقت میں جانوروں ب نظہارامکب کی ہاں کر گھر کے تاکہ اس نے ان لے سے کہا۔اور بول نے بچے کی کہا بھی ل،کلچے پہنچا یوڑی تلی کے فاریں بطہ روپ سے بہت حاصلٹیرے کا سونے کسی طرح کی آمے و س النے کرنے کا کیوں چڑے ہوا بیٹی ویلیم تقسیلاتے ہوئے وہ آسانس سے دو بیٹا میں پتا ایک آ کر تم خیال منہ تھا۔ان ابو کی رہے لوگ موبائیکل میں ول اور گر کوے اسی ہوئیں ڈالا شاہی جنگ منے کہ انتہادری اور بجیسے ہے۔وہ کیا۔میں جسٹ کہ کمرے پر پر جھل کے م'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"ایک دن\", max_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf03ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model counts for API...\n",
      "✓ Model counts saved to ../Phase_IV/api/model_counts.json\n",
      "  - Unigrams: 250\n",
      "  - Bigrams: 10232\n",
      "  - Trigrams: 69927\n"
     ]
    }
   ],
   "source": [
    "# Save model counts for API use\n",
    "print(\"Saving model counts for API...\")\n",
    "\n",
    "model_data = {\n",
    "    'unigram': {str(k): v for k, v in unigram_counts.items()},\n",
    "    'bigram': {','.join(map(str, k)): v for k, v in bigram_counts.items()},\n",
    "    'trigram': {','.join(map(str, k)): v for k, v in trigram_counts.items()}\n",
    "}\n",
    "\n",
    "with open('../Phase_IV/api/model_counts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ Model counts saved to ../Phase_IV/api/model_counts.json\")\n",
    "print(f\"  - Unigrams: {len(unigram_counts)}\")\n",
    "print(f\"  - Bigrams: {len(bigram_counts)}\")\n",
    "print(f\"  - Trigrams: {len(trigram_counts)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
